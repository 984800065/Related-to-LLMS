# PyTorch Transformer: RoPE & Learnable PE Support
A PyTorch-based Transformer model with configurable positional encoding: either Rotary Positional Embedding (RoPE) or learnable embeddings. Designed for tasks like function fitting or sequence modeling.
